# -*- coding: utf-8 -*-
"""ETL_Sreality_Scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a5IQCLwNVQhy4EglUs0bCg1fvkK-4Onp
"""

import tempfile
import shutil
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException

import pandas as pd
import time
import re
import psycopg2
import logging
from datetime import datetime, date
import supabase

'''
Great â€” you're ready to build a real data engineering (DE) pipeline that:

ðŸ“¤ Collects data automatically in the cloud

ðŸ’¾ Stores it reliably while your laptop is off

ðŸ§± Prepares you for real DE roles (hands-on experience with tools)
'''
#import libraries
#scrape
#download

#stack:
#selenium
#pandas
#postgreSQL
#github actions
#supabase
#airflow

#Parameters
# Config URLS & locators
initial_url = "https://www.sreality.cz/hledani/prodej/byty?vlastnictvi=osobni&razeni=nejlevnejsi&cena-do=3000000"
consent_url = "https://cmp.seznam.cz/nastaveni-souhlasu" # I don't need the rest of the url after ?
paginated_beginning_of_url = 'https://www.sreality.cz/hledani/prodej/byty?strana='
paginated_end_of_url = '&vlastnictvi=osobni&razeni=nejlevnejsi&cena-do=3000000'

driver = None
user_data_dir = None

logger = logging.getLogger(__name__)
# Setup headless Chrome
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox') # Essential for running Chrome in a containerized environment like Colab
options.add_argument('--disable-dev-shm-usage') # Overcomes limited resource problems
user_data_dir = None

#Initialize the Webdriver
driver = webdriver.Chrome(options=options)
wait = WebDriverWait(driver, 20)

# Building Shadow Root to click on consent button https://stackoverflow.com/questions/75992698/how-do-i-click-on-clickable-element-with-selenium-in-shadow-root-closed

# This INjects JS to make Shadow DOMs 'open' if they are 'closed
driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {'source': """
Element.prototype._attachShadow = Element.prototype.attachShadow;
Element.prototype.attachShadow = function () {
    return this._attachShadow( { mode: "open" } );
};
"""})


shadow_host_locator = (By.CSS_SELECTOR, '.szn-cmp-dialog-container')     # Locator for the Shadow DOM host element (the container for the consent dialog)
agree_button_shadow_locator = (By.CSS_SELECTOR, "button[data-testid='cw-button-agree-with-ads']")     # Locator for the "SouhlasÃ­m" button *inside* the Shadow DOM
expected_url_part = "https://www.sreality.cz/hledani/prodej/byty" # again I don't need the part after ?


driver.get(initial_url)

wait.until(EC.url_contains(consent_url)) # Waiting for the redirect

shadow_host = None

shadow_host = wait.until(EC.presence_of_element_located(shadow_host_locator)) # Time for the page to display the elements
shadow_root = driver.execute_script('return arguments[0].shadowRoot', shadow_host) # Execute JavaScript to get the Shadow Root of the found host element

# Now, find the button *within* the shadow_root
agree_button = WebDriverWait(shadow_root, 15).until(EC.element_to_be_clickable(agree_button_shadow_locator))
agree_button.click()

wait.until(EC.url_contains(expected_url_part))
print(driver.title)

wait = WebDriverWait(driver, 10) # Wait up to 10 seconds
last_li_element = wait.until(
        EC.presence_of_element_located((By.CSS_SELECTOR, 'nav[data-e2e="pagination"] > ul > li:last-child'))
    )

    # Get the 'class' attribute of the found element
number_of_pages = int(last_li_element.text)

#Scraping the real site
# the real scraper

counter = 0
rows = []

for page in range(1, number_of_pages + 1):
    if page < 1:
      driver.get(initial_url)

    else:

      try:
        paginated_url = f'{paginated_beginning_of_url}{page}{paginated_end_of_url}'
        driver.get(paginated_url); time.sleep(2)   # wait JS
      except ValueError as e:
        print(e)

    estate_items = driver.find_elements(By.CSS_SELECTOR, "li[id^='estate-list-item-']")

    for item in estate_items:
        title = item.find_element(By.CSS_SELECTOR, "p.css-d7upve").text
        location = item.find_elements(By.CSS_SELECTOR, "p.css-d7upve")[1].text
        price = item.find_element(By.CSS_SELECTOR, "p.css-ca9wwd").text
        link = item.find_element(By.CSS_SELECTOR, "a").get_attribute("href")
        #TODO scraped at

        counter +=1
        rows.append({
            "title": title,
            "location": location,
            "price": price,
            "link": link,
            "scraped": date.today().isoformat(),
            "page": page,
        })

        logger.info(f"Scraped page number{page}, found {counter} in overall number of {estate_items} estate items")

logger.info(f"Detected {number_of_pages} total pages.") #TODO delete if necessary

base_df = pd.DataFrame(rows)

#Wrap Up Session
driver.quit()

base_df.head()

base_df['title'] = base_df['title'].astype(str) # This also works
base_df.dtypes

#clean the data
base_df['price'] = (base_df['price'].str.replace(r"[^\d]", "",regex=True).astype(int))
base_df['location'] = base_df['location'].astype("string")
base_df['square_meters'] = base_df['title'].str.extract(r"(\d+)\s*m", expand=False).astype(int)
base_df['price_per_square_meter'] = (base_df['price'] / base_df['square_meters']).round(0).astype(int)

base_df.dtypes

base_df

#connection to Supabase
from supabase import create_client, Client

url = "https://hresoxzynnnjpgbpwzrk.supabase.co"
key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhyZXNveHp5bm5uanBnYnB3enJrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTExMzkwNjQsImV4cCI6MjA2NjcxNTA2NH0.PWyNnYPRRngnbKctMPROjFrprIo19kH8nmC2VMHVfpA'
supabase: Client = create_client(url, key)

try:
    # Attempt to fetch metadata (should raise no error)
    response = supabase.auth.get_session()
    print("âœ… Supabase client initialized successfully.")
except Exception as e:
    print("âŒ Connection failed:")
    print(e)


try:
    records = base_df.to_dict(orient="records")
    response = (
        supabase.table("listings")
        .insert(records)
        .execute()
    )
    print('Insert succesful')
    print(response.data)

except Exception as exception:
    print(exception)
